@article{LOPEZESPEJEL2023100013,
title = {A comprehensive review of State-of-The-Art methods for Java code generation from Natural Language Text},
journal = {Natural Language Processing Journal},
volume = {3},
pages = {100013},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000109},
author = {Jessica {López Espejel} and Mahaman Sanoussi {Yahaya Alassan} and El Mehdi Chouham and Walid Dahhane and El Hassane Ettifouri},
keywords = {Java code generation, Language models, Natural language processing, Recurrent neural networks, Transformer neural networks},
abstract = {Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers’ productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups: (1) encoder-only models, (2) decoder-only models, and (3) encoder–decoder models. In this paper, we provide a comprehensive review of the evolution and progress of deep learning models in Java code generation task. We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community. In addition, we provide a detailed description of datasets and evaluation metrics used in the literature. Finally, we discuss results of different models on CONCODE dataset, then propose some future directions.}
}